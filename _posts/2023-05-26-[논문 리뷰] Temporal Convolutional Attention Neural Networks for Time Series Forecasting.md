---
layout: post
categories: ë…¼ë¬¸ë¦¬ë·°
tags: IO, SensorData
comments: true
disqus: 'https-jisoo0-0-github-io' 
title: "[ë…¼ë¬¸ë¦¬ë·°] Temporal Convolutional Attention Neural Networks for Time Series Forecasting"
---


##### ë…¼ë¬¸ ë° ì‚¬ì§„ ì¶œì²˜
>Lin, Yang, Irena Koprinska, and Mashud Rana. "Temporal convolutional attention neural networks for time series forecasting." 2021 International joint conference on neural networks (IJCNN). IEEE, 2021.


<aside>
ğŸ’¡ íƒœì–‘ì—´ ê´€ë ¨ ì˜ˆì¸¡ ëª¨ë¸ ì œì•ˆ ì—°êµ¬

</aside>
# Abstract

- Temporal Convolutional Neural Networks(TCNNs)ì€ time series forecastingì„ í¬í•¨í•œ ì—¬ëŸ¬ê°€ì§€ sequence modelling taskì— ì ìš©ë˜ì–´ ì™”ìŒ.
- í•˜ì§€ë§Œ, ì¸í’‹ì´ ê¸´ ê²½ìš°, TCNNì€ ì—„ì²­ ë§ì€ ConvLayerì„ ìš”í•˜ë©° interpretable resultì„ ì œê³µí•˜ì§€ ëª»í•¨.
- ë³¸ ì—°êµ¬ì—ì„œëŠ” solar power forecastingì„ ìœ„í•œ TCAN ì„ ì œì•ˆí•¨
- TCAN
    - TCNNì˜ ê³„ì¸µì  conv êµ¬ì¡°ë¥¼ ì‚¬ìš©í•´ì„œ temporal dependenciesë¥¼ ì¶”ì¶œí•˜ê³ , sparse attentionì„ ì‚¬ìš©í•´ì„œ ì¤‘ìš”í•œ timestepë“¤ì„ ì¶”ì¶œí•¨
    - TCANì˜ sparse attention layerëŠ” extendëœ recpetive fieldë¥¼ ê°€ëŠ¥í•˜ê²Œ í•¨(deepí•œ êµ¬ì¡° í•„ìš” x)

# Introduction

- ARIMA ì™€ ê°™ì€ í†µê³„ì ì¸ ë°©ë²•ì€ well -establishedë˜ì—ˆê³  í­ë„“ê²Œ ì‚¬ìš©ë˜ì—ˆì§€ë§Œ ëª¨ë¸ì„ ì„ íƒí•  ë•Œ ë„ë©”ì¸ ì¡°ì‹ì„ ìš”í•˜ê³ , ê° ì‹œê³„ì—´ì„ ë…ë¦½ì ìœ¼ë¡œ fittingí•˜ë©° ê´€ë ¨ ì‹œê³„ì—´ì—ì„œ ê³µìœ í•˜ëŠ” íŒ¨í„´ì„ ì¶”ë¡ í•  ìˆ˜ ì—†ë‹¤ëŠ” ë‹¨ì ì´ ìˆìŒ.
- ë°˜ë©´, ë”¥ëŸ¬ë‹ ë°©ë²•ì€ time series forecastingì— ì—„ì²­ë‚˜ê²Œ ë§ì´ ì ìš©ë˜ì–´ ì™”ê³  promisingí•œ ê²°ê³¼ë„ ë³´ì—¬ì£¼ì—ˆìŒ. ë”¥ëŸ¬ë‹ ê¸°ë°˜ ë°©ë²•ë“¤ì€ rawdataë¥¼ less ë„ë©”ì¸ ì§€ì‹ì¸ ìƒíƒœë¡œ í•™ìŠµí•  ìˆ˜ ìˆê²Œí•˜ì˜€ê³  ë³µì¡í•œ íŒ¨í„´ì„ ì¶”ì¶œí•  ìˆ˜ ìˆê²Œ í–ˆìŒ
- TCNN
    - sequence modeling taskë¥¼ ìœ„í•´ì„œ spcifically design ëœ ëª¨ë¸ì„
    - casual conv, dilated conv, residual connectionì„ ì‚¬ìš©í•´ì„œ larger receptive filedë¥¼ êµ¬ì¶•í•˜ê³  unstableí•œ gradient problemì„ ì¶•ì†Œí•¨ê³¼ ë™ì‹œì—, í•™ìŠµ ì†ë„ì˜ í–¥ìƒì„ ì´ë£¨ê²Œ í•¨
    - í•˜ì§€ë§Œ ë§Œì•½ input sequenceê°€ ê¸¸ë‹¤ë©´ TCNNì€ temporal conv layerì„ í†µí•´ì„œ ì¶©ë¶„íˆ ë„“ì€ recpetive fieldë¥¼ êµ¬ì¶•í•´ì•¼í•  ê²ƒì„.
    - ì¶”ê°€ì ìœ¼ë¡œ TNCNì€ black-box êµ¬ì¡°ì´ê¸° ë•Œë¬¸ì— í•´ì„í•  ìˆ˜ ìˆëŠ” resultë¥¼ ì£¼ì§€ëŠ” ì•ŠìŒ
        - ê·¼ë° ë§ì€ time series forecastingì€ criticalí•œ ê²°ì •ë“¤ì„ í¬í•¨í•˜ê³  ìˆê³ , ê²°ì •ê¶Œìì—ê²Œ ì¶©ë¶„í•œ ì‹ ë¢°ë„ë¥¼ ì£¼ì–´ì•¼í•¨.
- ë³¸ ë…¼ë¬¸ì—ì„œëŠ”
    - Conv êµ¬ì¡°ì™€ attention ë°©ì‹ì´ í•©ì³ì§„ TCANì„ ì œì•ˆ.
        - TCANì€ ê³„ì¸µì  conv êµ¬ì¡°ë¥¼ í†µí•´ì„œ temporalí•œ dependencyë¥¼ í•™ìŠµí•˜ê³ , sparse attention layerì„ í†µí•´ì„œ forecasting resultì„ ìƒì„±í•¨
        - sparse attention layerì„ ì‚¬ìš©í•¨ìœ¼ë¡œì„œ ëª¨ë¸ì´ ëª¨ë“  historical input stepì— ì ‘ê·¼ê°€ëŠ¥í•˜ë„ë¡ í•˜ê³ , ê°€ì¥ ì¤‘ìš”í•œ timestepì— ì§‘ì¤‘í• ìˆ˜ ìˆë„ë¡ í•¨ê³¼ ë™ì‹œì— ê²°ê³¼ë¥¼ visualization í•´ì„œ í•´ì„ê°€ëŠ¥í•˜ê²Œ í•¨
    - TCANì„ ì„¸ê°€ì§€ í˜„ì‹¤ ì„¸ê³„ íƒœì–‘ì—´ ë°ì´í„°ì…‹ì„ í†µí•´ì„œ ê²€ì¦í–ˆê³  sotaë¥¼ ì°ìŒ

## Case Study : solar power forecasting

- íƒœì–‘ì—´ ì˜ˆì¸¡ì€ GENERATORì˜ ìµœì í™” ìŠ¤ì¼€ì¤„ë§ê³¼ SOLARì„ ELECTRICITY GRIDìœ¼ë¡œ í†µí•©í•˜ê¸° ìœ„í•´ì„œ í•„ìš”í•¨

### Data

- ë³¸ ì—°êµ¬ì—ì„œëŠ” Sanyo, Hanergy, Solarë°ì´í„°ë¥¼ ì‚¬ìš©í•¨
- Sanyoì™€ HanergyëŠ” í˜¸ì£¼ì— ìˆëŠ” 2ê°œì˜ PV plantìœ¼ë¡œë¶€í„° ë°œìƒëœ ë°ì´í„°ì„
    - ì˜¤ì „ 7ì‹œ ~ ì˜¤í›„ 5ì‹œ ì‚¬ì´ì˜ ë°ì´í„°ë§Œ ì‚¬ìš©ë˜ì—ˆìœ¼ë©°, 30ë¶„ ê°„ê²©ìœ¼ë¡œ ëª¨ì•„ì¡ŒìŒ
    - ë‘ ë°ì´í„°ì…‹ ëª¨ë‘ ë‚ ì”¨ì™€ ê¸°ìƒ ì˜ˆë³´ ë°ì´í„°ë„ ëª¨ë‘ ìˆ˜ì§‘ë˜ì—ˆìœ¼ë©° covariateìœ¼ë¡œ ì‚¬ìš©ë˜ì—ˆìŒ
- Solar ë°ì´í„°ëŠ” ì•Œë°”ë§ˆì™€ ë¯¸êµ­ì— ìˆëŠ” 137 PV pantì—ì„œ ìˆ˜ì§‘ëœ ë°ì´í„°ë¡œ, 1ì‹œê°„ ê°„ê²©ìœ¼ë¡œ ëª¨ì•„ì¡ŒìŒ
- ëª¨ë“  ë°ì´í„°ì…‹ì€ í‰ê·  0, unit ë¶„ì‚°ì„ ê°€ì§€ë„ë¡ ì •ê·œí™” ë˜ì—ˆìŒ

### Problem statement

![Untitled](http://drive.google.com/uc?export=view&id=1mTqf1b9h39isQQmrQWIAXcdPEmuQBsKL){: width="80%" height="80%"}{: .center}

![Untitled](http://drive.google.com/uc?export=view&id=1_Szri1iE6606yaxisRC8bVGkZImWW6o4){: width="80%" height="80%"}{: .center}

- step tì—ì„œì˜ inputì€ yi, t-1, xitì˜ concatì„
    - yi : ië²ˆì§¸ PV power generated at time t
    - xit: time-based ë©€í‹°ì°¨ì›ì˜ covariate vector

# Background

## Temporal Convolutional neural Network

- TCNNì€ 3ê°€ì§€ì˜ main techë¥¼ ì‚¬ìš©í•¨; causal conv, dilated conv, residual connection
    - Causal conv
        - tì—ì„œì˜ outputì€ ì´ì „ layerì˜ earlier time steps í˜¹ì€ time tì„ ì‚¬ìš©í•´ì„œë§Œ ë¶„í•´ë¨
        - zero paddingì´ hidden layerì— ì‚¬ìš©ë˜ì–´ì„œ hidden layerê°€ input layerì™€ ê°™ì€ ì°¨ì›ì„ ê°€ì ¸ì„œ convolutionì´ ìš©ì´í•  ìˆ˜ ìˆë„ë¡ í•¨
    - Dilated Conv
        - ë„“ì€ receptive fieldë¥¼ ê°€ëŠ¥í•˜ê²Œ í•˜ì˜€ê³  ê²°ê³¼ì ìœ¼ë¡œ ë¡±í…€ ë©”ëª¨ë¦¬ë¥¼ í¬ì°©í•  ìˆ˜ ìˆê²Œí–ˆìŒ
        - sequence element s ì—ì„œì˜ dilated conv operator FëŠ” ë‹¤ìŒê³¼ ê°™ì´ ì •ì˜ë¨
            
            ![Untitled](http://drive.google.com/uc?export=view&id=1DSQONXAgJ3SrrnIX0BFG_IOd82-0eRoa){: width="80%" height="80%"}{: .center}
            
            - fëŠ” conv filter
            - xëŠ” sequential í•œ input
            - këŠ” filter size
            - d ëŠ” dilation factor
            - Con kernelì€ ëª¨ë“  ë ˆì´ì–´ì—ëŒ€í•´ì„œ ê°™ì§€ë§Œ, dilation factorì€ ë„¤íŠ¸ì›Œí¬ì˜ ê¹Šì´ì— ë”°ë¼ì„œ ê¸°í•˜ê¸‰ìˆ˜ì ìœ¼ë¡œ ì¦ê°€í•¨.
    - Residual connection
        - Residual blockì€ gradient vanishing problemì„ í•´ê²°í•  ìˆ˜ ìˆë„ë¡ ë„ì›€. ì£¼ëœ ì•„ì´ë””ì–´ëŠ” xë¥¼ stacked layersì— inputí•´ì¤€ë‹¤ëŠ” ê²ƒ.
            
            ![Untitled](http://drive.google.com/uc?export=view&id=1yxkJDieUspVztdFt3FvIE66M_fvgLnB3){: width="80%" height="80%"}{: .center}
            
            ![Untitled](http://drive.google.com/uc?export=view&id=124a_3I4vcP82JspMENV4wntn6e48tyrn){: width="80%" height="80%"}{: .center}
            
            - ì¢Œ/ ìš° ë‘ê°€ì§€ì˜ ë¸Œëœì¹˜ê°€ ìˆëŠ”ë°, ë‘ê°€ì§€ì˜ widthê°€ ê°™ì§€ ì•Šê¸° ë•Œë¬¸ì—, 1*1 Cnvë¥¼ ì‚¬ìš©í•´ì„œ ìš°ì¸¡ ë¸Œëœì¹˜ì˜ widthë¥¼ ì¡°ì •í•´ì¤Œ
    - dilated convì„ ì´ìš©í•˜ë©´ ë” ë„“ì€ receptive fieldë¥¼ ê°€ì§ˆ ìˆ˜ ìˆì§€ë§Œ, long input sequenceë¥¼ íš¨ìœ¨ì ìœ¼ë¡œ ë‹¤ë£¨ì§€ëŠ” ëª»í•¨(ë” ë§ì€ ë ˆì´ì–´, ë” ë§ì€ í•™ìŠµì‹œê°„, ë” ë³µì¡í•œ êµ¬ì¡°)

## Attention Mechanism

- attention mechanismì€ ì›ë˜ seq2seq í…ŒìŠ¤í¬ë¥¼ í†µí•´ì„œ ì²˜ìŒ ì œì•ˆë˜ì—ˆìŒ
- ì´ëŠ” encoder-decoder í”„ë ˆì„ì›Œí¬ì— ì‚¬ìš©ë˜ì–´ì„œ, ìë™ìœ¼ë¡œ encoder input ì‹œí€€ìŠ¤ì—ì„œ decoder outputì— ë§ì€ ì˜í–¥ì„ ì£¼ëŠ” ë¶€ë¶„ì„ identifyí•  ìˆ˜ ìˆë„ë¡ í•¨
- seq2seq frameworkì—ì„œëŠ” encoderì™€ decoderê°€ sequentialí•œ stepì„ ê°€ì§€ê³  ë§¤ ì‹œì ë§ˆë‹¤ hidden stateì„ ìƒì„±í•˜ë„ë¡ í•¨.
- soft attentionì€ encoderì˜ hidden stateê³¼ decoderì˜ hidden stateì„ inputìœ¼ë¡œ ë°›ê³ , context vectorì„ ìƒì‚°í•´ëƒ„
    
    ![Untitled](http://drive.google.com/uc?export=view&id=1ajt7HBfoyZyydHAFENRYqNkvIhrkJzqG){: width="80%" height="80%"}{: .center}
    
- ì´í›„ softmaxí•¨ìˆ˜ë¥¼ í†µí•´ì„œ ì •ê·œí™”ë˜ê³  attention ê°€ì¤‘ì¹˜ë¥¼ ìƒì„±í•˜ê²Œ ë˜ë©°, ë§¤ encoderì˜ hidden state hiì— ëŒ€í•œ weight aiì˜ ì‹ì€ ì•„ë˜ì™€ ê°™ìŒ
    
    ![Untitled](http://drive.google.com/uc?export=view&id=1yu5EA8JOyNuEa1n0nyqKF0h2LZyE2wkU){: width="80%" height="80%"}{: .center}
    
    - ì´ ê°€ì¤‘ì¹˜ëŠ” encoder step i ê°€ decoder output setp tì— ëŒ€í•´ì„œ ê°€ ì–¼ë§ˆë‚˜ ì¤‘ìš”í•œì§€ë¥¼ ë‚˜íƒ€ëƒ„.
- ìµœì¢…ì ìœ¼ë¡œ attention layerì˜ outputì€ attention wieght ê³¼ encoder hidden stateì„ dot productí•´ì„œ ì–»ì–´ì§€ê²Œë˜ë©°, weighted outputì€ decoderì˜ hidden stateê³¼ concatë˜ì–´ì„œ decoderì˜ outputì„ ìƒì„±í•˜ê²Œ ë¨
- ì§ê´€ì ìœ¼ë¡œ ë³´ë©´, attention mechanismì€ decoderê°€ historical sequence stepì—ì„œ ê°€ì¥ ì¤‘ìš”í•œ ë¶€ë¶„ì— focusí•  ìˆ˜ ìˆë„ë¡ ë„ì™€ì£¼ê³ , seq2seqì˜ í•œê³„ì ì„ ê·¹ë³µí•  ìˆ˜ ìˆë„ë¡ í–ˆìŒ
- í•˜ì§€ë§Œ, softmaxë¥¼ ì´ìš©í•œ attentionì€ í•­ìƒ positiveí•œ attention weightì„ ëª¨ë“  timestepë§ˆë‹¤ ë‚´ë†“ëŠ”ë°, ì´ëŠ” long sequenceì— ì‚¬ìš©í•  ë•Œ ë¬´ê´€í•œ stepë„ í¬í•¨ë  ìˆ˜ ìˆë„ë¡ í•¨. ì´ë¥¼ ê·¹ë³µí•˜ê¸° ìœ„í•´ì„œ ìµœê·¼ì˜ ì—°êµ¬ë“¤ì€ sparse attention mechanism ì„ ì œì•ˆí•´ì„œ sparseí•œ attention mappingì„ ë°°ìš¸ ìˆ˜ ìˆë„ë¡ í–ˆìŒ.

# Temporal Convolutional Attention Neural Networks

## Motivation and Novelty

- TCAN aims to:
    - conv layerì˜ ê°œìˆ˜ë¥¼ ëŠ˜ë¦¬ì§€ ì•Šê³ ë„ large í•œ receptive fieldë¥¼ ê°€ëŠ¥í•˜ê²Œí•¨
    - input sequenceì˜ time step ì¤‘ ì¤‘ìš”í•œ ì •ë³´ì— ì§‘ì¤‘í•˜ê³  ë¶ˆí•„ìš”í•œ ì •ë³´ëŠ” ë¬´ì‹œí•¨
    - ê°€ì¥ ê´€ë ¨ìˆëŠ” timestepì— ëŒ€í•˜ì—¬ ì‹œê°í™”ë¥¼ ì œê³µí•¨
- TCNNì´ recpetive fieldë¥¼ ë„“íˆê¸° ìœ„í•´ì„œ expoentially dilated conv ë¥¼ ì‚¬ìš©í•¨ì—ë„ ë¶ˆêµ¬í•˜ê³ , ë§Œì•½ ì¸í’‹ ì‹œí€€ìŠ¤ê°€ ê¸¸ë‹¤ë©´, conv layerê°€ í•„ìš”í•˜ê²Œë˜ê³ , ì´ëŠ” í•™ìŠµ ì‹œê°„ ì¦ê°€ì™€ ë³µì¡ì„±ì„ ë†’ì´ê²Œ ë¨
- TCNNì´ ì˜ˆì¸¡í•˜ê¸° ìœ„í•´ì„œ í•„ìš”í•œ input stepì˜ ê°œìˆ˜ëŠ” Convì— ìˆëŠ” ëª¨ë“  effective í•œ historyë“¤ì„ í•©ì¹œ ê°œìˆ˜ì„
- Tlê¸¸ì´ë§Œí¼ì˜ input sequenceë¥¼ ì»¤ë²„í•˜ê¸° ìœ„í•œ receptive fildë¥¼ ìœ„í•´ì„œëŠ”, TCNNì€ ìµœì†Œí•œ nl ê°œì˜ conv layerê°€ í•„ìš”í•¨
    
    ![Untitled](http://drive.google.com/uc?export=view&id=1EKdh-s9eq9Ohs2-wb9Bj73gpnAh63_Yz){: width="80%" height="80%"}{: .center}
    
    - k = ì»¤ë„ ì‚¬ì´ì¦ˆ, dl = dilation factor, Tl : input sequence length, nl : conv layer ê°œìˆ˜
    - ë³¸ ì—°êµ¬ì—ì„œëŠ” tlì´ 20~24, k  = 3ì´ê¸° ë•Œë¬¸ì—, 4ê°œì˜ conv layerê°€ í•„ìš”í•¨

## Model Architecture

![Untitled](http://drive.google.com/uc?export=view&id=1DZCa0TqemtaFKGXgulwF5nUXOFaAXHBb){: width="80%" height="80%"}{: .center}

- TCANì€ ê·¸ë¦¼ê³¼ ê°™ì´ 3ê°œì˜ ë¶€ë¶„ìœ¼ë¡œ ë‚˜ëˆ„ì–´ì ¸ ìˆìŒ
    - temporal conv layer
    - sparse attention layer
    - output layer
- TCANì€ ê³„ì¸µì  conv êµ¬ì¡°ë¥¼ ì‚¬ìš©í•´ì„œ  input sequenceë¥¼ encodeí•˜ê³ , temporal patternì„ latent variableìœ¼ë¡œ ì¶”ì¶œí•¨
    - latent variableì€ attention mechanismì—ì„œ ê°€ì¥ relevantí•œ featureì„ í•™ìŠµí•˜ê³  final predictionì„ ìƒì„±í•˜ëŠ”ë°ì— ì‚¬ìš©ë¨
    - latent variableì€ ì „ì²´ input windowì˜ ì •ë³´ë¥¼ encodeí•˜ê³ , ì¶”ê°€ì ì¸ convì—†ì´ë„ ë„“ì€ receptive fieldë¥¼ ê°€ì§€ë„ë¡ í•¨

### Temporal Conv Layers

- TCANì€ temporal latent factor (ht-Tl:t)ë¥¼ multiple dilated temporal conv layer(TC)ë“¤ì„ í†µí•´ì„œ ì¶”ì¶œí•¨
    
    ![Untitled](http://drive.google.com/uc?export=view&id=1V8KDvNr7i9fN8XTwv9hnaur4ckVlo6o8){: width="80%" height="80%"}{: .center}
    
- ì¶”ì¶œëœ latent factorì€ intput sequenceì˜ ëª¨ë“  ì •ë³´ë¥¼ encodeí•¨

### Sparse Attention Layer

- temporal latent factors(ht-tl:t)ë¥¼ inputìœ¼ë¡œ ë°›ê³ , predcitionì„ ìœ„í•œ attention vector(ht)ë¥¼ ìƒì„±í•˜ë„ë¡ í•¨
- Transformerë‚˜ RNNêµ¬ì¡°ì— ì‚¬ìš©ë˜ì—ˆë˜ ì „í†µì ì¸ attention scoreì€ softmax í•¨ìˆ˜ë¡œ ê³„ì‚°ë˜ì—ˆìŒ. í•˜ì§€ë§Œ, softmaxëŠ” previous timestpë“¤ì— ëŒ€í•´ì„œ 0ì„ ì ˆëŒ€ ë¶€ì—¬í•˜ì§€ ì•Šê¸° ë•Œë¬¸ì—, ì¤‘ìš”í•˜ì§€ ì•Šì€ ë¶€ë¶„ë“¤ì„ ì™„ì „í•˜ê²Œ ë°°ì œí•˜ì§€ëŠ” ì•ŠìŒ
- ì‹œí€€ìŠ¤ ëª¨ë¸ë§ í…ŒìŠ¤í¬ì—ì„œëŠ” future timestepê°€ ëª‡ëª‡ê°œì˜ ì´ì „ timestepì— ê°•í•˜ê²Œ ê´€ë ¨ë˜ì–´ ìˆìŒ
    - ex. tì‹œì ì—ì„œì˜ solar powerëŠ” ê°™ì€ ë‚  ëª‡ì‹œê°„ ì „ ì‹œì ì´ë‚˜, ë‹¤ë¥¸ ë‚  ê°™ì€ ì‹œì ì˜ solar powerê³¼ ê°•í•˜ê²Œ ì—°ê´€ë˜ì–´ ìˆìŒ
        
        ![Untitled](http://drive.google.com/uc?export=view&id=1x72alOIqD4ptxzt-4D5sSA_WWMbouMQu){: width="80%" height="80%"}{: .center}
        
- ë³¸ ì—°êµ¬ì—ì„œëŠ” ì•ŒíŒŒ-entmax attentionì„ ì ìš©í–ˆê³ , ì•„ë˜ ì‹ê³¼ê°™ì´ ì •ì˜ë¨
    
    ![Untitled](http://drive.google.com/uc?export=view&id=1KRsRY55jCTyn_AemA6-y4GOlsro7E9kS){: width="80%" height="80%"}{: .center}
    
    - r : í°ê³±í•˜ê¸°
    - 1 : all-one vector
    - ì•ŒíŒŒ : í•˜ì´í¼íŒŒë¼ë¯¸í„°
    - ì•ŒíŒŒ-entmax ëŠ” ì•ŒíŒŒê°€ 1ì´ê³  softmaxì¼ ë•Œë‘ ì•ŒíŒŒê°€ 2ì´ê³  sparsemaxë¥¼ ì‚¬ìš©í• ë•Œ ë™ì¼í•¨
        
        ![Untitled](http://drive.google.com/uc?export=view&id=1iDsDpyHNCY2PE5WGMWK8EOVtKVugXJPn){: width="80%" height="80%"}{: .center}
        
        - ctëŠ” attention scoreê³¼ hidden stateì˜ dot productí•œ ê²°ê³¼ê°’

### Output layer

- output layerì—ì„œëŠ” attention vectorë“¤ì„ ì‚¬ìš©í•´ì„œ final predictionì„ ë§Œë“¦
- ë³¸ ì—°êµ¬ì—ì„œëŠ” ë°ì´í„°ê°€ ê°€ìš°ì‹œì•ˆ ë¶„í¬ë¥¼ ë”°ë¥¸ë‹¤ê³  ê°€ì •í•˜ëŠ”ë°, ê°€ìš°ì‹œì•ˆ ë¶„í¬ëŠ” real-world time series ëª¨ë¸ë§ ì‹œ ìì£¼ ì‚¬ìš©ë¨
- attention vectorì„ ë¶„í¬ì˜ í‰ê· ê³¼ varianceë¥¼ í¬í•¨í•˜ëŠ” ì˜ˆì¸¡ ê²°ê³¼ë¡œ transferí•¨
    
    ![Untitled](http://drive.google.com/uc?export=view&id=1D8Zgj9rw8Dw2b35wuB28w0qb8SZDYdwv){: width="80%" height="80%"}{: .center}
    
    - 11ë²ˆ ì‹ì€ varianceê°€ í•­ìƒ ì–‘ìˆ˜ê°€ ë˜ë„ë¡ ë§Œë“¤ì–´ì¤Œ
    - 10, 11ë²ˆ ì‹ì„ í†µí•´ì„œ ê°€ìš°ì‹œì•ˆ ë¶„í¬ë¥¼ í˜•ì„±í•˜ê³ , ë¶„í¬ì—ì„œ predictionì´ ìƒ˜í”Œë§ ë  ìˆ˜ ìˆê²Œë¨

### Loss Function

- ì•„ë˜ ì‹ì„ í†µí•´ì„œ loss ê°€ ìµœì†Œí™”ë¨
    
    ![Untitled](http://drive.google.com/uc?export=view&id=1baGEc7KUEy5sVpZQ_Oal7WoOTNp7FwDo){: width="80%" height="80%"}{: .center}
    
    - y^ëŠ” point ì˜ˆì¸¡
    - pointì™€ probabilistic ì˜ˆì¸¡ì„ ëª¨ë‘ ë‹¤ ì •í™•í•˜ê¸° ìœ„í•´ì„œ MAE ì™€ Negative Log-Likelihood(NLL)ì„ í•©ì³ì„œ ì‚¬ìš©í•¨.
    - ì •ê·œí™” íŒŒë¼ë¯¸í„° aë¥¼ ì‚¬ìš©í•˜ì˜€ê³ , aê°€ ì»¤ì§ˆìˆ˜ë¡ probabilistic forecastì˜ ê°€ì¤‘ì¹˜ê°€ ë†’ì•„ì§

# Experimental Setup

## Methods used for comparison

- TCGAN ì„ Deep AR, N-Beats-G, N-Beats-I, LogSparse Transformer, TCNNê³¼ ë¹„êµí•¨
    - Deep AR
        - ê´‘ë²”ìœ„í•˜ê²Œ ì´ìš©ë˜ëŠ” seq2seq probabilistic ì˜ˆì¸¡ ëª¨ë¸
    - N-BEATS
        - backward ì™€ forward residual linkë“¤ê³¼ fully connected layerë“¤ì˜ stackë“¤ì— ê¸°ë°˜í•œ ëª¨ë¸
        - N-BEATS-GëŠ” generic forecasting ê²°ê³¼ë¥¼ , N-BEATS-IëŠ” interpretable í•œ ê²°ê³¼ë¥¼ ë³´ì—¬ì¤Œ
    - LongSparse Transformer
        - ìµœê·¼ì— ì œì•ˆëœ transformerì˜ time series forecastingì— ë§ë„ë¡ ë³€í˜•ëœ ëª¨ë¸
    - TCNN
        - ìƒˆë¡œìš´ conv êµ¬ì¡°
    - Persistence
        - typical baseline in forecasting
        - next dayì˜ predictionì„ ìœ„í•´ì„œ ì´ì „ ë‚ ì§œì˜ time series ë¥¼ ê³ ë ¤í•¨

### Data Split and Hyperparameter Tuning

- ì•„ë‹´ ì˜µí‹°ë§ˆì´ì €ë¥¼ ì‚¬ìš©í–ˆê³ , mini batch gradient descentë¥¼ ì‚¬ìš©í•´ì„œ ìµœì í™”ë˜ì—ˆìœ¼ë©° ì—í­ì€ 200ìœ¼ë¡œ ì„¤ì •ë¨
- ë² ì´ì§€ì•ˆ optimizationì„ í•˜ì´í¼íŒŒë¼ë¯¸í„° searchë¡œ ì„¤ì •í•˜ì˜€ìœ¼ë©°, ìµœëŒ€ iterationì€ 20ìœ¼ë¡œ ì„¤ì •í•¨
- ë¹„êµëŒ€ìƒì— ìˆëŠ” ëª¨ë¸ë“¤ì€ ì›ë³¸ ë…¼ë¬¸ì— ì œì•ˆëœ ë°©ë²•ëŒ€ë¡œ íŒŒì¸íŠœë‹ë¨
- Sanyoì™€Hanergyë°ì´í„°ì…‹ì— ëŒ€í•´ì„œ, ë§ˆì§€ë§‰ ë…„ë„ì— í•´ë‹¹í•˜ëŠ” ë°ì´í„°ëŠ” test , ë‘ë²ˆì§¸ ë§ˆì§€ë§‰ë…„ë„ì— í•´ë‹¹í•˜ëŠ” ë°ì´í„°ëŠ” validation set, ë‚¨ì€ ë°ì´í„°ë“¤ì€ trainingì— ì‚¬ìš©í•¨
- TCANì—ì„œ lrì€ 0.005ë¡œ ê³ ì •ë˜ì–´ ìˆì—ˆê³  ë°°ì¹˜ì‚¬ì´ì¦ˆëŠ” Sanyoì—ì„œëŠ” 256, Hanergyì™€ Solarì—ì„œëŠ” 512ë¡œì„¤ì •ë¨. ì•ŒíŒŒëŠ” 1.5, ì •ê·œí™” íŒŒë¼ë¯¸í„°ëŠ” 0.5ë¡œ ì„¤ì •ë¨. dropoutrateì€ 0,0.1,0.2ì¤‘í•˜ë‚˜ë¡œ ì„ íƒë˜ì—ˆê³  ì»¤ë„ì‚¬ì´ì¦ˆëŠ” 3,4ì¤‘ í•˜ë‚˜ë¡œ ì„ íƒë˜ì—ˆìŒ.

### Evaluation Measures

- Accuracy results
    
    ![Untitled](http://drive.google.com/uc?export=view&id=1cGsTGa4iVyOKeA0cTICEW4iTBi_loaIt){: width="80%" height="80%"}{: .center}
    
    - Persistenceì™€ N-Beatsê°€ probabilistic forecastfmf todtjdgkwl dksgrl Eoansdp, 0.5 loss(==MAE loss)ë§Œ ê²°ê³¼ê°€ ë‚˜ì˜´
    - Point forecastì—ì„œëŠ”
- ë‘ê°œì˜ TCNN ëª¨ë¸ì„ ë¹„êµí–ˆì„ë•Œ, TCNN-4ê°€ TCNN-3ë³´ë‹¤ ìš°ì›”í•œ ê²ƒì„ ì•Œ ìˆ˜ ìˆìŒ
    - TCANì€ ë‘ê°œì˜ ëª¨ë¸ë³´ë‹¤ ë” ì •í™•í•˜ê³ , ìœ„ì—ì„œ ì–¸ê¸‰í–ˆë˜ ì •í™•ë„ í–¥ìƒ ë° receptive filedì˜ í™•ì¥ì„ ê°€ëŠ¥í•˜ê²Œ í–ˆìŒ(Convì¶”ê°€ ì—†ì´)
    - TCANê³¼ TCNN-4ëª¨ë‘ ì¸í’‹ ì‹œí€€ìŠ¤ë¥¼ ì»¤ë²„í•  ìˆ˜ ìˆì§€ë§Œ, TCANì´ ë” ì ì€ Convë¥¼ ì‚¬ìš©í•¨
- 2ê°œì˜ consecuative daysì˜ ì˜ˆì¸¡ ê²°ê³¼
- consecuative
    
    ![Untitled](http://drive.google.com/uc?export=view&id=1pDTFPPK_PVEDDzV6P7s1TvEVk33kYEJL){: width="80%" height="80%"}{: .center}
    
    - ì¢Œì¸¡ì€ actual vs predicted values for each day
        - ì–¼ë§ˆë‚˜ ì˜ ì˜ˆì¸¡í–ˆëŠ”ì§€ë¥¼ ë³´ì—¬ì¤Œ
    - ìš°ì¸¡ì€ corresponding sparse attention map
        - ì´ê±´ pair attention scoreì„ ë³´ì—¬ì£¼ëŠ”ë°, ì´ì „ time series stepì¤‘ì— ë¯¸ë˜ë¥¼ ì˜ˆì¸¡í•˜ëŠ”ë°ì— ì¤‘ìš”ë„ë¥¼ ë‚˜íƒ€ëƒ„
        - ê³¼ê±°ì™€ ë¯¸ë˜ stepì— ëŒ€í•œ ì˜ì¡´ì„±ì´ sparseí•˜ë‹¤ëŠ” ê²ƒì„ ë³´ì—¬ì£¼ë©°, ë” ê¸´ ê³¼ê±°ì— accessí•  ìˆ˜ ìˆë‹¤ëŠ”ê²ƒì´ ì¤‘ìš”ã…ë‹¤ëŠ” ê²ƒì„ ë³´ì—¬ì£¼ì—ˆìŒ
            - EX. ëª¨ë“  MAPì´ TIME STEPSì˜ ì´ë¥¸ ì‹œì ì—ì„œëŠ” ì—„ì²­ ë†’ì€ ATTENTION SCOREì„ ë³´ì—¬ì¤Œ
                - ì²«ë²ˆì§¸ future predictionì€ second input stepìœ¼ë¡œë§Œ ê²°ì •ë¨
    - TCANê³¼ TCNN-4ì˜ í•™ìŠµ ì†ë„ë¥¼ ë¹„êµí–ˆì„ ë•Œ, TCANì´ ìƒëŒ€ì ìœ¼ë¡œ ë¹ ë¥¸ í•™ìŠµì„ ë³´ì—¬ì£¼ì—ˆìŒ
        
        ![Untitled](http://drive.google.com/uc?export=view&id=14_bLE3gLjecVXN636nLWsyFfk9KcpvQl){: width="80%" height="80%"}{: .center}
<br/>
<br/>
<div id="disqus_thread"></div>
<script>
    (function() { // DON'T EDIT BELOW THIS LINE
    var d = document, s = d.createElement('script');
    s.src = 'https://https-jisoo0-0-github-io.disqus.com/embed.js';
    s.setAttribute('data-timestamp', +new Date());
    (d.head || d.body).appendChild(s);
    })();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>